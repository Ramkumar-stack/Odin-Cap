{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramkumar-stack/Odin-Cap/blob/main/1Gen1_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyFHZQYCUozF",
        "outputId": "c33cbbcd-a25d-487a-86d2-c3dd61bd1eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.4.1\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "9e44af78674845898db20729f0a5892e"
          ]
        },
        "id": "ateemJfcU-lG",
        "outputId": "54f1c056-a23a-4c7c-903d-21b973e12622"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e44af78674845898db20729f0a5892e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX7eHKQGaAzR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V_nVr3wibeO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zaQOl_Yaz_d",
        "outputId": "feab0420-626b-456c-d138-845a333ff1b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, all of the nations to this day are united under one nation, and the one kingdom, which is the one with its full glory, will be with God at the name of Christ, and his Son shall be glorified.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a text generation pipeline\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Generate text\n",
        "response = generator(\n",
        "    \"Once upon a time\",\n",
        "    max_length=50,\n",
        "    num_return_sequences=1,\n",
        "    pad_token_id=50256,  # Explicitly set pad_token_id to eos_token_id\n",
        "    truncation=True      # Explicitly enable truncation\n",
        ")\n",
        "\n",
        "# Print the generated text\n",
        "print(response[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ColQRkuys2iM",
        "outputId": "549e89bc-cad7-448f-c390-74571f2e52db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['once', 'upon', 'a', 'time']\n",
            "once upon a time there was a princess </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import Vocabulary\n",
        "\n",
        "# Download the punkt tokenizer models\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"Once upon a time\",\n",
        "    \"There was a little girl\",\n",
        "    \"She lived in a small house\",\n",
        "    \"Once upon a time there was a princess\",\n",
        "    \"The princess lived in a castle\"\n",
        "]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_text = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "\n",
        "# Prepare data for the n-gram model\n",
        "train, vocab = padded_everygram_pipeline(3, tokenized_text)\n",
        "model = Laplace(3)  # Trigram model with Laplace smoothing\n",
        "model.fit(train, vocab)\n",
        "\n",
        "# Generate text\n",
        "def generate_text(model, seed_text, max_length=50):\n",
        "    words = nltk.word_tokenize(seed_text.lower())\n",
        "    print(words)\n",
        "    for _ in range(max_length):\n",
        "        ngram = tuple(words[-2:])  # Get last two words for trigram prediction\n",
        "        #print(ngram)\n",
        "        next_word = model.generate(text_seed=ngram)\n",
        "        #print(next_word)\n",
        "        if next_word:\n",
        "            words.append(next_word)\n",
        "        else:\n",
        "            break\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"Once upon a time\"\n",
        "generated_text = generate_text(model, seed_text)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSc-JmsIrMXo",
        "outputId": "0ca84100-d578-4ebc-ee76-ed21bd43ca35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['once', 'upon', 'a', 'time'], ['there', 'was', 'a', 'little', 'girl'], ['she', 'lived', 'in', 'a', 'small', 'house'], ['once', 'upon', 'a', 'time', 'there', 'was', 'a', 'princess'], ['the', 'princess', 'lived', 'in', 'a', 'castle']]\n",
            "Train (padded n-grams):\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'once'), ('<s>',), ('<s>', 'once'), ('<s>', 'once', 'upon'), ('once',), ('once', 'upon'), ('once', 'upon', 'a'), ('upon',), ('upon', 'a'), ('upon', 'a', 'time'), ('a',), ('a', 'time'), ('a', 'time', '</s>'), ('time',), ('time', '</s>'), ('time', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'there'), ('<s>',), ('<s>', 'there'), ('<s>', 'there', 'was'), ('there',), ('there', 'was'), ('there', 'was', 'a'), ('was',), ('was', 'a'), ('was', 'a', 'little'), ('a',), ('a', 'little'), ('a', 'little', 'girl'), ('little',), ('little', 'girl'), ('little', 'girl', '</s>'), ('girl',), ('girl', '</s>'), ('girl', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'she'), ('<s>',), ('<s>', 'she'), ('<s>', 'she', 'lived'), ('she',), ('she', 'lived'), ('she', 'lived', 'in'), ('lived',), ('lived', 'in'), ('lived', 'in', 'a'), ('in',), ('in', 'a'), ('in', 'a', 'small'), ('a',), ('a', 'small'), ('a', 'small', 'house'), ('small',), ('small', 'house'), ('small', 'house', '</s>'), ('house',), ('house', '</s>'), ('house', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'once'), ('<s>',), ('<s>', 'once'), ('<s>', 'once', 'upon'), ('once',), ('once', 'upon'), ('once', 'upon', 'a'), ('upon',), ('upon', 'a'), ('upon', 'a', 'time'), ('a',), ('a', 'time'), ('a', 'time', 'there'), ('time',), ('time', 'there'), ('time', 'there', 'was'), ('there',), ('there', 'was'), ('there', 'was', 'a'), ('was',), ('was', 'a'), ('was', 'a', 'princess'), ('a',), ('a', 'princess'), ('a', 'princess', '</s>'), ('princess',), ('princess', '</s>'), ('princess', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'the'), ('<s>',), ('<s>', 'the'), ('<s>', 'the', 'princess'), ('the',), ('the', 'princess'), ('the', 'princess', 'lived'), ('princess',), ('princess', 'lived'), ('princess', 'lived', 'in'), ('lived',), ('lived', 'in'), ('lived', 'in', 'a'), ('in',), ('in', 'a'), ('in', 'a', 'castle'), ('a',), ('a', 'castle'), ('a', 'castle', '</s>'), ('castle',), ('castle', '</s>'), ('castle', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "\n",
            "Vocabulary:\n",
            "Vocabulary words: ['<s>', '<s>', 'once', 'upon', 'a', 'time', '</s>', '</s>', '<s>', '<s>', 'there', 'was', 'a', 'little', 'girl', '</s>', '</s>', '<s>', '<s>', 'she', 'lived', 'in', 'a', 'small', 'house', '</s>', '</s>', '<s>', '<s>', 'once', 'upon', 'a', 'time', 'there', 'was', 'a', 'princess', '</s>', '</s>', '<s>', '<s>', 'the', 'princess', 'lived', 'in', 'a', 'castle', '</s>', '</s>']\n",
            "['once', 'upon', 'a', 'time']\n",
            "('a', 'time')\n",
            "</s>\n",
            "('time', '</s>')\n",
            "</s>\n",
            "('</s>', '</s>')\n",
            "</s>\n",
            "('</s>', '</s>')\n",
            "</s>\n",
            "('</s>', '</s>')\n",
            "</s>\n",
            "once upon a time </s> </s> </s> </s> </s>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import Vocabulary\n",
        "\n",
        "# Download the punkt tokenizer models\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"Once upon a time\",\n",
        "    \"There was a little girl\",\n",
        "    \"She lived in a small house\",\n",
        "    \"Once upon a time there was a princess\",\n",
        "    \"The princess lived in a castle\"\n",
        "]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_text = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "print(tokenized_text)\n",
        "# Prepare data for the n-gram model\n",
        "train, vocab = padded_everygram_pipeline(3, tokenized_text)\n",
        "\n",
        "# Convert and print train data\n",
        "print(\"Train (padded n-grams):\")\n",
        "for ngrams_list in train:\n",
        "    print(list(ngrams_list))  # Convert each generator to a list\n",
        "\n",
        "# Print the vocabulary\n",
        "print(\"\\nVocabulary:\")\n",
        "print(\"Vocabulary words:\", list(vocab))\n",
        "\n",
        "# Generate text\n",
        "def generate_text(model, seed_text, max_length=5):\n",
        "    words = nltk.word_tokenize(seed_text.lower())\n",
        "    print(words)\n",
        "    for _ in range(max_length):\n",
        "        ngram = tuple(words[-2:])  # Get last two words for trigram prediction\n",
        "        print(ngram)\n",
        "        next_word = model.generate(text_seed=ngram)\n",
        "        print(next_word)\n",
        "        if next_word:\n",
        "            words.append(next_word)\n",
        "        else:\n",
        "            break\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"Once upon a time\"\n",
        "generated_text = generate_text(model, seed_text)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqxqSoa_VM52",
        "outputId": "692d6b31-0d05-49f6-a6cb-46cc2d20a397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Encode input text\n",
        "input_text = \"Once upon a time\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a text generation pipeline\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Generate text (Generator is the function)\n",
        "response = generator(\n",
        "    \"Once upon a time\",\n",
        "    max_length=50,\n",
        "    num_return_sequences=1,\n",
        "    pad_token_id=50256,  # Explicitly set pad_token_id to eos_token_id (code for dot is 50256,)\n",
        "    truncation=True      # Explicitly enable truncation to remove any type of <,> signs\n",
        ")\n",
        "\n",
        "# Print the generated text\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXsYCa6PxBG_",
        "outputId": "2d8ea2be-f59a-4f66-bd55-e87e4940dc8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, this particular point was important to make. This was in March 1917 when Himmler had arrived, and he had arrived with a very strong hand to prevent Stalin ever getting his back.\n",
            "\n",
            "But while he was waiting for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Components of GPT-2\n",
        "GPT (Generative Pre-trained Transformer)\n",
        "Variants and Sizes\n",
        "Training and Use Cases\n",
        "1. GPT (Generative Pre-trained Transformer)\n",
        "GPT: GPT is a family of language models based on the transformer architecture. GPT-2 is the second version of this model, following GPT. It uses a transformer-based architecture for generating coherent and contextually relevant text.\n",
        "\n",
        "Auto-Regressive Model: GPT-2 generates text in an auto-regressive manner. This means it predicts the next word in a sequence based on the words that precede it. This approach allows GPT-2 to generate text that continues from a given prompt in a coherent way.\n",
        "\n",
        "Transformer Architecture: GPT-2 uses the transformer architecture, which consists of multiple layers of self-attention and feed-forward networks. This allows it to capture long-range dependencies and generate contextually appropriate text.\n",
        "\n",
        "2. Variants and Sizes\n",
        "GPT-2 comes in several sizes, each with different numbers of parameters and capacities:\n",
        "\n",
        "GPT-2 Small: 117 million parameters\n",
        "GPT-2 Medium: 345 million parameters\n",
        "GPT-2 Large: 762 million parameters\n",
        "GPT-2 XL: 1.5 billion parameters\n",
        "The size of the model impacts its ability to generate more coherent and contextually relevant text.\n",
        "\n",
        "3. Training and Use Cases\n",
        "Training: GPT-2 was trained on a diverse dataset that includes text from books, articles, and websites. It was trained using unsupervised learning, where the model learns to predict the next word in a sentence given the previous words.\n",
        "\n",
        "Use Cases: GPT-2 is used for a variety of natural language processing tasks, including:\n",
        "\n",
        "Text generation\n",
        "Conversational agents\n",
        "Text completion\n",
        "Creative writing\n",
        "Code generation"
      ],
      "metadata": {
        "id": "sv9yr8AjxSTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cwJ0kBbExTeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "#sentiment_pipeline = pipeline('sentiment-analysis', model='gpt2')\n",
        "# Analyze sentiment\n",
        "response = sentiment_pipeline(\"I do not like using the transformers library!\")\n",
        "\n",
        "# Print the sentiment analysis result\n",
        "print(response[0]['label'])\n",
        "print(response[0]['score'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fm2FrSqxBMY",
        "outputId": "9c2f67b4-b891-4929-f416-7898d8f9ed61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEGATIVE\n",
            "0.993513286113739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "# Perform sentiment analysis on some text\n",
        "texts = [\n",
        "    \"I love this product!\",\n",
        "    \"This is the worst experience I've ever had.\"\n",
        "]\n",
        "\n",
        "results = sentiment_pipeline(texts)\n",
        "\n",
        "for text, result in zip(texts, results):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\")\n"
      ],
      "metadata": {
        "id": "JO3T3vKaxBOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "463d5b6d-7c62-4ee2-820e-0d60a3e2a3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love this product!\n",
            "Sentiment: POSITIVE, Score: 0.9999\n",
            "Text: This is the worst experience I've ever had.\n",
            "Sentiment: NEGATIVE, Score: 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Import the Pipeline: The pipeline function from the transformers library provides a high-level interface for various NLP tasks.\n",
        "\n",
        "Load the Model: The pipeline function loads the specified model and tokenizer for sentiment analysis.\n",
        "\n",
        "Perform Sentiment Analysis: The classifier object is used to analyze the sentiment of the provided texts. It returns a list of dictionaries containing the sentiment label and confidence score for each text.\n",
        "\n",
        "Print the Results: The results are printed, showing the sentiment label (e.g., POSITIVE or NEGATIVE) and the confidence score for each input text."
      ],
      "metadata": {
        "id": "0MEfNQKXx4FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "#sentiment_pipeline = pipeline('sentiment-analysis', model='gpt2')\n",
        "# Analyze sentiment\n",
        "response = sentiment_pipeline(\"sad happy \")\n",
        "\n",
        "# Print the sentiment analysis result\n",
        "print(response[0])\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "#sentiment_pipeline = pipeline('sentiment-analysis', model='gpt2')\n",
        "# Analyze sentiment\n",
        "response = sentiment_pipeline(\"sad happy \")\n",
        "\n",
        "# Print the sentiment analysis result\n",
        "print(response[0])\n",
        "{'label': 'POSITIVE', 'score': 0.9998382329940796}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY8WfsijxBRh",
        "outputId": "7faf0be2-986d-429b-d3b1-c0201d31b32a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'POSITIVE', 'score': 0.9998382329940796}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'POSITIVE', 'score': 0.9998382329940796}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'POSITIVE', 'score': 0.9998382329940796}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3rRTrNUdxBUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gc1Fo93GxBWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7_mhRaqYxBZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}