{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramkumar-stack/Odin-Cap/blob/main/3Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-txa9kTQQTAx",
        "outputId": "e69c0c53-1f30-4718-b2dd-e95b1b13a54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEGATIVE\n",
            "0.993513286113739\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "#sentiment_pipeline = pipeline('sentiment-analysis', model='gpt2')\n",
        "# Analyze sentiment\n",
        "response = sentiment_pipeline(\"I do not like using the transformers library!\")\n",
        "\n",
        "# Print the sentiment analysis result\n",
        "print(response[0]['label'])\n",
        "print(response[0]['score'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "# Perform sentiment analysis on some text\n",
        "texts = [\n",
        "    \"I love this product!\",\n",
        "    \"This is the worst experience I've ever had.\"\n",
        "]\n",
        "\n",
        "results = sentiment_pipeline(texts)\n",
        "\n",
        "for text, result in zip(texts, results):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox345ZIkRA0Y",
        "outputId": "7a05b926-003c-4a29-9bb3-61c1a29fd275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love this product!\n",
            "Sentiment: POSITIVE, Score: 0.9999\n",
            "Text: This is the worst experience I've ever had.\n",
            "Sentiment: NEGATIVE, Score: 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Import the Pipeline: The pipeline function from the transformers library provides a high-level interface for various NLP tasks.\n",
        "\n",
        "Load the Model: The pipeline function loads the specified model and tokenizer for sentiment analysis.\n",
        "\n",
        "Perform Sentiment Analysis: The classifier object is used to analyze the sentiment of the provided texts. It returns a list of dictionaries containing the sentiment label and confidence score for each text.\n",
        "\n",
        "Print the Results: The results are printed, showing the sentiment label (e.g., POSITIVE or NEGATIVE) and the confidence score for each input text."
      ],
      "metadata": {
        "id": "DFNgg7jeRJbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model 'distilbert-base-uncased-finetuned-sst-2-english' is a distilled version of BERT that has been fine-tuned for sentiment analysis using the SST-2 dataset. Here’s a detailed breakdown of each component:\n",
        "\n",
        "Components of the Model\n",
        "DistilBERT\n",
        "Base\n",
        "Uncased\n",
        "Fine-Tuning\n",
        "SST-2 Dataset\n",
        "1.\n",
        "DistilBERT: DistilBERT is a smaller, faster, and more efficient version of BERT. It retains most of BERT’s performance while being more computationally efficient. DistilBERT is created through a process called \"knowledge distillation,\" where a smaller model (the student) learns to mimic the behavior of a larger pre-trained model (the teacher). This allows it to achieve similar results to BERT but with fewer parameters and faster inference times.\n",
        "2.\n",
        "Base: This refers to the size of the model. In the context of DistilBERT, it follows the same architecture as BERT-Base, which includes 6 layers (transformer blocks), 768 hidden units per layer, and 12 attention heads. Although DistilBERT is smaller than BERT-Base, it is still powerful for many NLP tasks.\n",
        "3.\n",
        "Uncased: This model does not differentiate between uppercase and lowercase letters. All text is converted to lowercase before processing. This approach can simplify the model and reduce the complexity of the text representation.\n",
        "4.\n",
        "Fine-Tuned: The model has been fine-tuned on a specific dataset for a particular task. In this case, it has been fine-tuned for sentiment analysis, meaning it has been trained to classify text into sentiment categories, such as positive or negative.\n",
        "5. SST-2 Dataset\n",
        "SST-2 (Stanford Sentiment Treebank 2): This dataset is used for sentiment analysis and contains movie reviews labeled with positive or negative sentiments. SST-2 is part of the larger Stanford Sentiment Treebank, which includes more detailed sentiment labels.\n"
      ],
      "metadata": {
        "id": "fUiZ0EHJfTMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "#sentiment_pipeline = pipeline('sentiment-analysis', model='gpt2')\n",
        "# Analyze sentiment\n",
        "response = sentiment_pipeline(\"sad happy \")\n",
        "\n",
        "# Print the sentiment analysis result\n",
        "print(response[0])Import the Pipeline: The pipeline function from the transformers library provides a high-level interface for various NLP tasks.\n",
        "\n",
        "Load the Model: The pipeline function loads the specified model and tokenizer for sentiment analysis.\n",
        "\n",
        "Perform Sentiment Analysis: The classifier object is used to analyze the sentiment of the provided texts. It returns a list of dictionaries containing the sentiment label and confidence score for each text.\n",
        "\n",
        "Print the Results: The results are printed, showing the sentiment label (e.g., POSITIVE or NEGATIVE) and the confidence score for each input text.\n",
        "\n",
        "The model 'distilbert-base-uncased-finetuned-sst-2-english' is a distilled version of BERT that has been fine-tuned for sentiment analysis using the SST-2 dataset. Here’s a detailed breakdown of each component:\n",
        "\n",
        "Components of the Model DistilBERT Base Uncased Fine-Tuning SST-2 Dataset 1. DistilBERT: DistilBERT is a smaller, faster, and more efficient version of BERT. It retains most of BERT’s performance while being more computationally efficient. DistilBERT is created through a process called \"knowledge distillation,\" where a smaller model (the student) learns to mimic the behavior of a larger pre-trained model (the teacher). This allows it to achieve similar results to BERT but with fewer parameters and faster inference times. 2. Base: This refers to the size of the model. In the context of DistilBERT, it follows the same architecture as BERT-Base, which includes 6 layers (transformer blocks), 768 hidden units per layer, and 12 attention heads. Although DistilBERT is smaller than BERT-Base, it is still powerful for many NLP tasks. 3. Uncased: This model does not differentiate between uppercase and lowercase letters. All text is converted to lowercase before processing. This approach can simplify the model and reduce the complexity of the text representation. 4. Fine-Tuned: The model has been fine-tuned on a specific dataset for a particular task. In this case, it has been fine-tuned for sentiment analysis, meaning it has been trained to classify text into sentiment categories, such as positive or negative.\n",
        "\n",
        "SST-2 Dataset SST-2 (Stanford Sentiment Treebank 2): This dataset is used for sentiment analysis and contains movie reviews labeled with positive or negative sentiments. SST-2 is part of the larger Stanford Sentiment Treebank, which includes more detailed sentiment labels.\n",
        "\n",
        "[ ]\n",
        "12345678910\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "#sentiment_pipeline = pipeline('sentiment-analysis', model='gpt2')\n",
        "# Analyze sentiment\n",
        "response = sentiment_pipeline(\"sad happy \")\n",
        "\n",
        "# Print the sentiment analysis result\n",
        "print(response[0])\n",
        "{'label': 'POSITIVE', 'score': 0.9998382329940796}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0M7alZQHcD8",
        "outputId": "abffb3fa-b84c-4e58-8761-4337759c0068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'POSITIVE', 'score': 0.9998382329940796}\n"
          ]
        }
      ]
    }
  ]
}